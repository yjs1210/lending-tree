{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\James\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "###Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML\n",
    "import datetime\n",
    "import math\n",
    "import scipy.optimize as optimize\n",
    "import statistics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "from scipy import stats\n",
    "import scipy\n",
    "import warnings\n",
    "from scipy.stats import norm\n",
    "from sklearn import datasets, linear_model, cross_validation\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn import ensemble\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import KFold \n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('ggplot')\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "import xgboost\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### data loading \n",
    "df = pd.read_csv('./data/loan.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### data loading & cleaning\n",
    "df = df.drop_duplicates(subset='id').reset_index(drop=True)\n",
    "df.replace([np.inf, -np.inf], np.nan)\n",
    "df = df.dropna(subset=['id', 'funded_amnt','int_rate','installment','grade'])\n",
    "df = df[np.isfinite(df['annual_inc']) & np.isfinite(df['int_rate']) &  np.isfinite(df['funded_amnt'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###changing data type to appropriate date type \n",
    "df['last_pymnt_d']=pd.to_datetime(df.last_pymnt_d)\n",
    "df['issue_d']=pd.to_datetime(df.issue_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###fill missing last_payments(no payments) with issue date, may want a separete column for this treatment\n",
    "###This is for calculation of IRRs later\n",
    "df['last_pymnt_d'].fillna(df.issue_d,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Some pre-processing for ease of calculations\n",
    "df['issue_yr'] = df.issue_d.dt.year\n",
    "df['issue_mo']= df.issue_d.dt.month\n",
    "df['last_pymnt_yr'] = df.last_pymnt_d.dt.year\n",
    "df['last_pymnt_mo']= df.last_pymnt_d.dt.month\n",
    "df['mo_diff'] = pd.to_numeric((df['last_pymnt_yr'] - \n",
    "                          df['issue_yr'])*12 + df['last_pymnt_mo'] -df['issue_mo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Flag for completed loans\n",
    "searchfor = ['Fully Paid', 'Charged Off', 'Default']\n",
    "defaults = ['Charged Off', 'Default']\n",
    "df['loan_completion_flag']=  np.where(df['loan_status'].str.contains('|'.join(searchfor)) ,1, np.nan)\n",
    "###Flag for fully paid loans\n",
    "df['fully_paid'] = np.where(df['loan_status'].str.contains('Fully Paid') ,1, \n",
    "                                  np.where(df['loan_status'].str.contains('|'.join(defaults)) ,0,np.nan))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Average payment = Total payment - recoveries - last payment amount over the life -1 month of the investment\n",
    "df['avg_pymnt'] = (df['total_pymnt']-df['recoveries']-df['last_pymnt_amnt'])/(np.maximum((df['mo_diff']-1),0))\n",
    "###Treating infinites that appear when there is no payment or only 1 payment \n",
    "df['avg_pymnt'] = (df['avg_pymnt']).replace(np.Inf,0)\n",
    "df['avg_pymnt'] = (df['avg_pymnt']).replace(-np.Inf,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###IRR calculations\n",
    "###Input: a row of a dataframe with lending data \n",
    "def irr_calc(x):  \n",
    "    ##varible initialization\n",
    "    initial_invest = -x['funded_amnt']\n",
    "    avg_payment = x['avg_pymnt']\n",
    "    num_payments = np.max(int(x['mo_diff'])-1,0)\n",
    "    recovery = x['recoveries'] -x['collection_recovery_fee']\n",
    "    recovery_duration = np.maximum(36 - num_payments + 1 + 12,12)\n",
    "    avg_recovery = recovery/recovery_duration\n",
    "    last_payment_amount = x['last_pymnt_amnt']\n",
    "    ###IRR calculation, input: series of cash flows, total payment - recoveries\n",
    "    ###evenly divided and spread across the life of the loan and finally recovery and chargeoff fees\n",
    "    return ((np.irr([initial_invest]+[avg_payment]*num_payments + [last_payment_amount] +\n",
    "                    [avg_recovery]*recovery_duration)+1)**12-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Calculating at a row level, individual security IRRs. Method will be faulty for loans that didn't mature.\n",
    "###Warning: the calculation takes a fair amount of time ~few minutes\n",
    "df['irr']=df.apply(irr_calc, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##NaNs returned from IRRs with 0 payments should be -100% return \n",
    "df['irr']=df['irr'].replace(np.NaN,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Filter down to completed loans and has at least 36 months of possible history\n",
    "df_filtered = df[df['loan_status'].str.contains('|'.join(searchfor))].query(\"term == ' 36 months' and issue_yr <=2012\").copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##making grade flags\n",
    "grade_flags = pd.get_dummies(df_filtered.grade) \n",
    "home_flag = pd.get_dummies(df_filtered.home_ownership) \n",
    "purpose_flag = pd.get_dummies(df_filtered.purpose)\n",
    "df_filtered=pd.concat([df_filtered,grade_flags,home_flag,purpose_flag], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_train_error(X_train, y_train, model):\n",
    "    '''returns in-sample error for already fit model.'''\n",
    "    predictions = model.predict(X_train)\n",
    "    mse = mean_squared_error(y_train, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return mse\n",
    "    \n",
    "def calc_validation_error(X_test, y_test, model):\n",
    "    '''returns out-of-sample error for already fit model.'''\n",
    "    predictions = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return mse\n",
    "    \n",
    "def calc_metrics(X_train, y_train, X_test, y_test, model):\n",
    "    '''fits model and returns the RMSE for in-sample error and out-of-sample error'''\n",
    "    model.fit(X_train, y_train)\n",
    "    train_error = calc_train_error(X_train, y_train, model)\n",
    "    validation_error = calc_validation_error(X_test, y_test, model)\n",
    "    return train_error, validation_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns= [\"int_rate\",\"annual_inc\",\"funded_amnt\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"MORTGAGE\",\"NONE\",\n",
    "          \"OTHER\",\"OWN\",\"RENT\",\"car\",\"credit_card\",\"debt_consolidation\",\"educational\",\"home_improvement\",\"house\",\n",
    "         \"major_purchase\",\"medical\",\"moving\",\"other\",\"renewable_energy\",\"small_business\",\"vacation\",\"wedding\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last [post](http://jameslee.posthaven.com/peer-to-peer-lending-markets-part-2-using-linear-regression-and-gradient-boosting-regression-to-outperform-the-market), gbm regression model and linear regression were pitted against random loan selection method to see if we can outperform a well diversified market portfolio, and we saw that our models dramatically outperformed the market portfolio. So with that exploratory work out of the way, I will aim to build a model with extensive tuning that can potentially be used in practice. We will split our data into train and test sets and further split the train set using k-fold cross validation to tune our parameters. Our final model will be compared vs other models using the test set that was held out.\n",
    "\n",
    "For feature selection ,we will first feed in all plausible features, calculate their importances using GBM model's built-in feature importance function. Using this as a ranked order, we will deduct bottom 20-th percentile features out of our model, then see the model's performance using that many features across different validation sets using 5 fold cross-validation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, we tune the parameters and feature selection at the same time, but this is not feasible for my computation resources. So we will first feed all the features, get the right feature numbers, then tune our parameters using those number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = cross_validation.train_test_split(df_filtered,test_size=.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first establish a standard set of parameters and train an initial model to calcualte feature importance across all the features on our training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.01, loss='ls', max_depth=4, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=500, presort='auto', random_state=None,\n",
       "             subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 2,\n",
    "          'learning_rate': 0.01, 'loss': 'ls'}\n",
    "clf = ensemble.GradientBoostingRegressor(**params)\n",
    "clf.fit(df_train[columns], df_train['irr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = clf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then remove 20% of the features at a time then see how the new models perform using k-fold cross validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=4) # Define the split - into 4\n",
    "all_trials = []\n",
    "for i in range(int(np.ceil(len(columns)/6))):\n",
    "    sorted_features = importance.argsort()[-(len(importance)-i*6):][::-1]\n",
    "    for index, (train, test) in enumerate(kf.split(df_train)):\n",
    "        x_train = df_train.iloc[train][columns].iloc[:,sorted_features]\n",
    "        y_train = df_train.iloc[train]['irr']\n",
    "        y_test = df_train.iloc[test]['irr']\n",
    "        x_test = df_train.iloc[test][columns].iloc[:,sorted_features]\n",
    "\n",
    "        params = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 2,\n",
    "              'learning_rate': 0.01, 'loss': 'ls'}\n",
    "\n",
    "        clf = ensemble.GradientBoostingRegressor(**params)\n",
    "        clf.fit(x_train, y_train)\n",
    "        gbm_predictions = clf.predict(x_test)\n",
    "        df_temp = pd.DataFrame({'irr':y_test, 'gbm_predictions':gbm_predictions})\n",
    "        gbm_returns = np.mean(df_temp.nlargest(1000, 'gbm_predictions')['irr'])\n",
    "        all_trials.append({\n",
    "            'n_features': len(x_train.columns),\n",
    "            'mse': calc_train_error(x_test,y_test,clf),\n",
    "            'returns': gbm_returns\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output=pd.DataFrame.from_records(all_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mse</th>\n",
       "      <th>returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_features</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.045378</td>\n",
       "      <td>0.108145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.045230</td>\n",
       "      <td>0.107996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.045176</td>\n",
       "      <td>0.110360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.045175</td>\n",
       "      <td>0.110418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.045167</td>\n",
       "      <td>0.110371</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 mse   returns\n",
       "n_features                    \n",
       "5           0.045378  0.108145\n",
       "11          0.045230  0.107996\n",
       "17          0.045176  0.110360\n",
       "23          0.045175  0.110418\n",
       "29          0.045167  0.110371"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.groupby(['n_features'])['mse','returns'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our cross validation yields the lowest error as well as the higest returns with n_features = 17. We will be using this as a basis for our hyperparameter tuning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our set of parameters that we will be using. Now it's time to tune our parameters and fit a model on our entire training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lower learning rate will always yield to bette result. But our constraint is computation time. So we can't set it too low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52411"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"loss\":[\"ls\"],\n",
    "    \"learning_rate\": [0.05],\n",
    "    \"min_samples_split\":[300],\n",
    "    \"min_samples_leaf\": [50],\n",
    "    \"max_depth\":[8],\n",
    "    \"max_features\":[\"sqrt\"],\n",
    "    \"subsample\":[.8],\n",
    "    \"n_estimators\":[25,50, 100, 200, 500, 1000]\n",
    "    }\n",
    "clf = GridSearchCV(ensemble.GradientBoostingRegressor(), parameters, cv=10, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"n_estimators\":[25,50, 100, 200, 500, 1000]\n",
    "    }\n",
    "clf_ntrees = GridSearchCV(ensemble.GradientBoostingRegressor(learning_rate=0.05, min_samples_split=300,\n",
    "                                                      min_samples_leaf=50,max_depth=8,max_features='sqrt',\n",
    "                                                      subsample=0.8,random_state=10), parameters, cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.05, loss='ls', max_depth=8,\n",
       "             max_features='sqrt', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=50, min_samples_split=300,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "             presort='auto', random_state=10, subsample=0.8, verbose=0,\n",
       "             warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'n_estimators': [25, 50, 100, 200, 500, 1000]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_ntrees.fit(df_train[columns].iloc[:,importance.argsort()[-17:][::-1]], df_train['irr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mean: 0.01574, std: 0.00071, params: {'n_estimators': 25}, mean: 0.01832, std: 0.00117, params: {'n_estimators': 50}, mean: 0.01881, std: 0.00170, params: {'n_estimators': 100}, mean: 0.01730, std: 0.00199, params: {'n_estimators': 200}, mean: 0.01091, std: 0.00266, params: {'n_estimators': 500}, mean: 0.00098, std: 0.00256, params: {'n_estimators': 1000}] {'n_estimators': 100} 0.0188129230061\n"
     ]
    }
   ],
   "source": [
    "print(clf_ntrees.grid_scores_, clf_ntrees.best_params_, clf_ntrees.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 60,  70,  80,  90, 100, 110, 120, 130, 140, 150])"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " np.linspace(60, 150, 10).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"n_estimators\": np.linspace(60, 150, 10).astype(int)\n",
    "    }\n",
    "clf_ntrees2 = GridSearchCV(ensemble.GradientBoostingRegressor(learning_rate=0.05, min_samples_split=300,\n",
    "                                                      min_samples_leaf=50,max_depth=8,max_features='sqrt',\n",
    "                                                      subsample=0.8,random_state=10), parameters, cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.05, loss='ls', max_depth=8,\n",
       "             max_features='sqrt', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=50, min_samples_split=300,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "             presort='auto', random_state=10, subsample=0.8, verbose=0,\n",
       "             warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'n_estimators': array([ 60,  70,  80,  90, 100, 110, 120, 130, 140, 150])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_ntrees2.fit(df_train[columns].iloc[:,importance.argsort()[-17:][::-1]], df_train['irr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 80}\n"
     ]
    }
   ],
   "source": [
    "print( clf_ntrees2.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "80 is the best n_trees we will move onto other parameters. The order of tuning variables should be decided carefully. You should take the variables with a higher impact on outcome first. For instance, max_depth and min_samples_split have a significant impact and weâ€™re tuning those first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters2= {\n",
    "    'max_depth':  np.linspace(4, 16, 7).astype(int), 'min_samples_split': np.linspace(200,1000, 5).astype(int)\n",
    "}\n",
    "clf_max_min = GridSearchCV(ensemble.GradientBoostingRegressor(learning_rate=0.05, \n",
    "                                                      min_samples_leaf=50,max_features='sqrt',\n",
    "                                                      subsample=0.8,random_state=10, n_estimators = 80), parameters2, cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.05, loss='ls', max_depth=3,\n",
       "             max_features='sqrt', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=50, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=80, presort='auto',\n",
       "             random_state=10, subsample=0.8, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'max_depth': array([ 4,  6,  8, 10, 12, 14, 16]), 'min_samples_split': [200, 400, 600, 800, 987, 1000]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_max_min.fit(df_train[columns].iloc[:,importance.argsort()[-17:][::-1]], df_train['irr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 6, 'min_samples_split': 400}\n"
     ]
    }
   ],
   "source": [
    "print(clf_max_min.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 30.,  40.,  50.,  60.,  70.,  80.])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(30,80,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters3= {\n",
    "    'max_depth':  np.linspace(5,7,3).astype(int), 'min_samples_split': np.linspace(300,500, 5).astype(int), 'min_samples_leaf': np.linspace(30,80,6).astype(int)\n",
    "}\n",
    "clf_max_min_leaves = GridSearchCV(ensemble.GradientBoostingRegressor(learning_rate=0.05, \n",
    "                                                      max_features='sqrt',\n",
    "                                                      subsample=0.8,random_state=10, n_estimators = 80), parameters3, cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.05, loss='ls', max_depth=3,\n",
       "             max_features='sqrt', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=80, presort='auto',\n",
       "             random_state=10, subsample=0.8, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'max_depth': array([5, 6, 7]), 'min_samples_split': array([300, 350, 400, 450, 500]), 'min_samples_leaf': array([30, 40, 50, 60, 70, 80])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_max_min_leaves.fit(df_train[columns].iloc[:,importance.argsort()[-17:][::-1]], df_train['irr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.018795155337357913,\n",
       " {'max_depth': 7, 'min_samples_leaf': 40, 'min_samples_split': 400})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_max_min_leaves.best_score_ , clf_max_min_leaves.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n",
       "       14., 15., 16., 17.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(1,17,17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters4= {\n",
    "    'max_features':  np.linspace(1,17,17).astype(int)\n",
    "}\n",
    "clf_max_min_leaves_feature = GridSearchCV(ensemble.GradientBoostingRegressor(learning_rate=0.05, \n",
    "                                                      max_depth = 7, min_samples_leaf = 40, min_samples_split = 400,\n",
    "                                                      subsample=0.8,random_state=10, n_estimators = 80), parameters4, cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_max_min_leaves_feature.fit(df_train[columns].iloc[:,importance.argsort()[-17:][::-1]], df_train['irr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.019032377455621013, {'max_features': 6})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_max_min_leaves_feature.best_score_ , clf_max_min_leaves_feature.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters5= {\n",
    "    'subsample':  np.linspace(.5,1,6)\n",
    "}\n",
    "clf_5 = GridSearchCV(ensemble.GradientBoostingRegressor(learning_rate=0.05, max_features=6,\n",
    "                                                      max_depth = 7, min_samples_leaf = 40, min_samples_split = 400,\n",
    "                                                      random_state=10, n_estimators = 80), parameters5, cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.019032377455621013, {'subsample': 0.8})"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_5.fit(df_train[columns].iloc[:,importance.argsort()[-17:][::-1]], df_train['irr'])\n",
    "clf_5.best_score_ , clf_5.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(1000,4000,4).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 100.,  490.,  880., 1270., 1660., 2050., 2440., 2830., 3220.,\n",
       "       3610., 4000.])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(100,4000,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters6= {\n",
    "    'n_estimators':  np.linspace(100,4000,11).astype(int)\n",
    "}\n",
    "clf_6= GridSearchCV(ensemble.GradientBoostingRegressor(learning_rate=0.01, max_features=6,\n",
    "                                                      max_depth = 7, min_samples_leaf = 40, min_samples_split = 400,\n",
    "                                                      random_state=10,subsample=.8), parameters6, cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.01710893682071948, {'n_estimators': 80})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_6.fit(df_train[columns].iloc[:,importance.argsort()[-17:][::-1]], df_train['irr'])\n",
    "clf_6.best_score_ , clf_6.best_params_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
