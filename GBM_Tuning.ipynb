{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\James\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "###Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML\n",
    "import datetime\n",
    "import math\n",
    "import scipy.optimize as optimize\n",
    "import statistics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "from scipy import stats\n",
    "import scipy\n",
    "import warnings\n",
    "from scipy.stats import norm\n",
    "from sklearn import datasets, linear_model, cross_validation\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn import ensemble\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import KFold \n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('ggplot')\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "import xgboost\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### data loading \n",
    "df = pd.read_csv('./data/loan.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### data loading & cleaning\n",
    "df = df.drop_duplicates(subset='id').reset_index(drop=True)\n",
    "df.replace([np.inf, -np.inf], np.nan)\n",
    "df = df.dropna(subset=['id', 'funded_amnt','int_rate','installment','grade'])\n",
    "df = df[np.isfinite(df['annual_inc']) & np.isfinite(df['int_rate']) &  np.isfinite(df['funded_amnt'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###changing data type to appropriate date type \n",
    "df['last_pymnt_d']=pd.to_datetime(df.last_pymnt_d)\n",
    "df['issue_d']=pd.to_datetime(df.issue_d)\n",
    "df['earliest_cr_line']= pd.to_datetime(df.earliest_cr_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###fill missing last_payments(no payments) with issue date, may want a separete column for this treatment\n",
    "###This is for calculation of IRRs later\n",
    "df['last_pymnt_d'].fillna(df.issue_d,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Some pre-processing for ease of calculations\n",
    "df['issue_yr'] = df.issue_d.dt.year\n",
    "df['issue_mo']= df.issue_d.dt.month\n",
    "df['last_pymnt_yr'] = df.last_pymnt_d.dt.year\n",
    "df['last_pymnt_mo']= df.last_pymnt_d.dt.month\n",
    "df['mo_diff'] = pd.to_numeric((df['last_pymnt_yr'] - \n",
    "                          df['issue_yr'])*12 + df['last_pymnt_mo'] -df['issue_mo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Credit History Length\n",
    "df['cl_yr'] = df.earliest_cr_line.dt.year\n",
    "df['cl_mo']= df.earliest_cr_line.dt.month\n",
    "df['cl_hist'] = pd.to_numeric((df['issue_yr'] - \n",
    "                          df['cl_yr'])*12 + df['issue_mo'] -df['cl_mo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Flag for completed loans\n",
    "searchfor = ['Fully Paid', 'Charged Off', 'Default']\n",
    "defaults = ['Charged Off', 'Default']\n",
    "df['loan_completion_flag']=  np.where(df['loan_status'].str.contains('|'.join(searchfor)) ,1, np.nan)\n",
    "###Flag for fully paid loans\n",
    "df['fully_paid'] = np.where(df['loan_status'].str.contains('Fully Paid') ,1, \n",
    "                                  np.where(df['loan_status'].str.contains('|'.join(defaults)) ,0,np.nan))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Average payment = Total payment - recoveries - last payment amount over the life -1 month of the investment\n",
    "df['avg_pymnt'] = (df['total_pymnt']-df['recoveries']-df['last_pymnt_amnt'])/(np.maximum((df['mo_diff']-1),0))\n",
    "###Treating infinites that appear when there is no payment or only 1 payment \n",
    "df['avg_pymnt'] = (df['avg_pymnt']).replace(np.Inf,0)\n",
    "df['avg_pymnt'] = (df['avg_pymnt']).replace(-np.Inf,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "###IRR calculations\n",
    "###Input: a row of a dataframe with lending data \n",
    "def irr_calc(x):  \n",
    "    ##varible initialization\n",
    "    initial_invest = -x['funded_amnt']\n",
    "    avg_payment = x['avg_pymnt']\n",
    "    num_payments = np.max(int(x['mo_diff'])-1,0)\n",
    "    recovery = x['recoveries'] -x['collection_recovery_fee']\n",
    "    recovery_duration = np.maximum(36 - num_payments + 1 + 12,12)\n",
    "    avg_recovery = recovery/recovery_duration\n",
    "    last_payment_amount = x['last_pymnt_amnt']\n",
    "    ###IRR calculation, input: series of cash flows, total payment - recoveries\n",
    "    ###evenly divided and spread across the life of the loan and finally recovery and chargeoff fees\n",
    "    return ((np.irr([initial_invest]+[avg_payment]*num_payments + [last_payment_amount] +\n",
    "                    [avg_recovery]*recovery_duration)+1)**12-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Calculating at a row level, individual security IRRs. Method will be faulty for loans that didn't mature.\n",
    "###Warning: the calculation takes a fair amount of time ~few minutes\n",
    "df['irr']=df.apply(irr_calc, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##NaNs returned from IRRs with 0 payments should be -100% return \n",
    "df['irr']=df['irr'].replace(np.NaN,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Filter down to completed loans and has at least 36 months of possible history\n",
    "df_filtered = df[df['loan_status'].str.contains('|'.join(searchfor))].query(\"term == ' 36 months' and issue_yr <=2012\").copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##making grade flags\n",
    "grade_flags = pd.get_dummies(df_filtered.grade) \n",
    "home_flag = pd.get_dummies(df_filtered.home_ownership) \n",
    "purpose_flag = pd.get_dummies(df_filtered.purpose)\n",
    "emp_length_flag = pd.get_dummies(df_filtered.emp_length)\n",
    "verification_flag = pd.get_dummies(df_filtered.verification_status)\n",
    "df_filtered=pd.concat([df_filtered,grade_flags,home_flag,purpose_flag,emp_length_flag,verification_flag], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns= [\"int_rate\",\"annual_inc\",\"dti\",\"funded_amnt\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"MORTGAGE\",\"NONE\",\n",
    "          \"OTHER\",\"OWN\",\"RENT\",\"car\",\"credit_card\",\"debt_consolidation\",\"educational\",\"home_improvement\",\"house\",\n",
    "         \"major_purchase\",\"medical\",\"moving\",\"other\",\"renewable_energy\",\"small_business\",\"vacation\",\"wedding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_all = [\"int_rate\",\"annual_inc\",\"dti\",\"funded_amnt\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"MORTGAGE\",\"NONE\",\n",
    "          \"OTHER\",\"OWN\",\"RENT\",\"car\",\"credit_card\",\"debt_consolidation\",\"educational\",\"home_improvement\",\"house\",\n",
    "         \"major_purchase\",\"medical\",\"moving\",\"other\",\"renewable_energy\",\"small_business\",\"vacation\",\"wedding\",\"cl_hist\",'1 year', '10+ years', '2 years', '3 years', '4 years', '5 years',\n",
    "       '6 years', '7 years', '8 years', '9 years', '< 1 year', 'Not Verified', 'Source Verified', 'Verified' ,'pub_rec','revol_bal','revol_util']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_train_error(X_train, y_train, model):\n",
    "    '''returns in-sample error for already fit model.'''\n",
    "    predictions = model.predict(X_train)\n",
    "    mse = mean_squared_error(y_train, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return mse\n",
    "    \n",
    "def calc_validation_error(X_test, y_test, model):\n",
    "    '''returns out-of-sample error for already fit model.'''\n",
    "    predictions = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return mse\n",
    "    \n",
    "def calc_metrics(X_train, y_train, X_test, y_test, model):\n",
    "    '''fits model and returns the RMSE for in-sample error and out-of-sample error'''\n",
    "    model.fit(X_train, y_train)\n",
    "    train_error = calc_train_error(X_train, y_train, model)\n",
    "    validation_error = calc_validation_error(X_test, y_test, model)\n",
    "    return train_error, validation_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection is a critical step in machine learning. It allows the model to generalize better on datasets that it hasn't seen while training and improve overall accuracy by helping it learn from true signals rather than noise. However, a lot of modern techniques such as GBM has built-in regularization functionality that makes feature selection unnecessary at times. If we aggressively remove variables that have incremental utility in prediction despite their short-comings such as multicollinearity, cumulatively it can cause a significant negative effect in our model's prediction power.\n",
    "\n",
    "When a problem is as complicated and opaque as feature selection can be, we can utilize sampling and cross-validation to get unbiased distribution of performances across different methodologies that can be used to compare different models. For this exercise, I continue looking at the peer to peer lending data and aim to build a GBM model that can outperform the linear model, which defeated the GBM model in the last [post](http://jameslee.posthaven.com/peer-to-peer-lending-markets-part-2-using-linear-regression-and-gradient-boosting-regression-to-outperform-the-market). By comparing models via cross-validation, one that utilizes feature selection versus the one that doesn't, we can be rest assured that we didn't leave any stones unturned. \n",
    "\n",
    "There are many ways to engineer and select features, but two diametrically opposite approaches are used in conjunction. One is through inspection, winnowing down our features to ones that intuitively make sense in predicting loan performance, and the other is a blackbox approach using feature importance metric and sampling through cross-validation. \n",
    "\n",
    "Below are all the data available at our disposal. Some engineering was done to clean up the data and transform them into a machine-readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'member_id', 'loan_amnt', 'funded_amnt', 'funded_amnt_inv',\n",
      "       'term', 'int_rate', 'installment', 'grade', 'sub_grade', 'emp_title',\n",
      "       'emp_length', 'home_ownership', 'annual_inc', 'verification_status',\n",
      "       'issue_d', 'loan_status', 'pymnt_plan', 'url', 'desc', 'purpose',\n",
      "       'title', 'zip_code', 'addr_state', 'dti', 'delinq_2yrs',\n",
      "       'earliest_cr_line', 'inq_last_6mths', 'mths_since_last_delinq',\n",
      "       'mths_since_last_record', 'open_acc', 'pub_rec', 'revol_bal',\n",
      "       'revol_util', 'total_acc', 'initial_list_status', 'out_prncp',\n",
      "       'out_prncp_inv', 'total_pymnt', 'total_pymnt_inv', 'total_rec_prncp',\n",
      "       'total_rec_int', 'total_rec_late_fee', 'recoveries',\n",
      "       'collection_recovery_fee', 'last_pymnt_d', 'last_pymnt_amnt',\n",
      "       'next_pymnt_d', 'last_credit_pull_d', 'collections_12_mths_ex_med',\n",
      "       'mths_since_last_major_derog', 'policy_code', 'application_type',\n",
      "       'annual_inc_joint', 'dti_joint', 'verification_status_joint',\n",
      "       'acc_now_delinq', 'tot_coll_amt', 'tot_cur_bal', 'open_acc_6m',\n",
      "       'open_il_6m', 'open_il_12m', 'open_il_24m', 'mths_since_rcnt_il',\n",
      "       'total_bal_il', 'il_util', 'open_rv_12m', 'open_rv_24m', 'max_bal_bc',\n",
      "       'all_util', 'total_rev_hi_lim', 'inq_fi', 'total_cu_tl', 'inq_last_12m',\n",
      "       'issue_yr', 'issue_mo', 'last_pymnt_yr', 'last_pymnt_mo', 'mo_diff',\n",
      "       'loan_completion_flag', 'fully_paid', 'avg_pymnt', 'irr', 'cl_yr',\n",
      "       'cl_mo', 'cl_hist'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some obvious candidates that can explain loan returns are debt-to-income ratio, annual income, interest rate, loan size and loan grade. Loan grade is an encapsulation of many factors that can affect the credibility of the borrower such as their credit history. Lending tree defines their loan grade as follows. \n",
    "\n",
    "\"The loan grade is the result of a formula that takes into account not only credit score, but also a combination of several indicators of credit risk from the credit report and loan application. All loans have either a 36- or 60-month term, with fixed interest rates and equal payments.\"\n",
    "\n",
    "We can infer that credit score is a huge factor into their loan grade scoring. Let's look at the components of the credit score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://www.myfico.com/static/images/education/ce_FICO-Score-chart.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://www.myfico.com/static/images/education/ce_FICO-Score-chart.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we need to take away is that since the loan grade captures a lot of our potential features, it's important to remove features such as credit history in our feature sets to reduce multicollinearity in our model, but we will include other variables such as loan purpose which can capture insights that loan grade cannot. For example, a loan taken out to service a credit card debt is more likely to default than loans taken out for education.\n",
    "\n",
    "All in all, these are the features that will be included in the model. We perform hot one encoding to convert categorical variables into a feature that our algorithm can understand. We will further narrow these features down using feature importance and threshold method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['int_rate',\n",
      " 'annual_inc',\n",
      " 'dti',\n",
      " 'funded_amnt',\n",
      " 'A',\n",
      " 'B',\n",
      " 'C',\n",
      " 'D',\n",
      " 'E',\n",
      " 'F',\n",
      " 'G',\n",
      " 'MORTGAGE',\n",
      " 'NONE',\n",
      " 'OTHER',\n",
      " 'OWN',\n",
      " 'RENT',\n",
      " 'car',\n",
      " 'credit_card',\n",
      " 'debt_consolidation',\n",
      " 'educational',\n",
      " 'home_improvement',\n",
      " 'house',\n",
      " 'major_purchase',\n",
      " 'medical',\n",
      " 'moving',\n",
      " 'other',\n",
      " 'renewable_energy',\n",
      " 'small_business',\n",
      " 'vacation',\n",
      " 'wedding']\n"
     ]
    }
   ],
   "source": [
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cols = columns_all.copy()\n",
    "get_cols.append('irr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_dropped = df_filtered.copy()[get_cols].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74874, 74751)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_filtered), len(df_filtered_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = cross_validation.train_test_split(df_filtered_dropped,test_size=.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first establish a standard set of parameters and train an initial model to calculate feature importance, which is a built-in metric provided by GBM algorithm that tells you how influential the features were. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.01, loss='ls', max_depth=4, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=500, presort='auto', random_state=None,\n",
       "             subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 2,\n",
    "          'learning_rate': 0.01, 'loss': 'ls'}\n",
    "clf = ensemble.GradientBoostingRegressor(**params)\n",
    "clf.fit(df_train[columns], df_train['irr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importance = clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.17455462,  0.22307073,  0.15930349,  0.12287988,  0.00057328,\n",
       "        0.01162901,  0.0129365 ,  0.02576675,  0.0157758 ,  0.03267067,\n",
       "        0.03357703,  0.00174292,  0.        ,  0.00055117,  0.00626372,\n",
       "        0.00123417,  0.00950443,  0.0307704 ,  0.01770299,  0.01006844,\n",
       "        0.00315873,  0.00353244,  0.0116074 ,  0.01252817,  0.00241643,\n",
       "        0.01510553,  0.00625816,  0.04810297,  0.00182496,  0.00488921])"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature importance is graphed below. Interest rate, annual income, debt-to-income ratio and loan sizes are the most importance features. Loans taken out for small business, credit card payment and some of the loan grade features were also important. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(range(len(columns)),importance)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>int_rate</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>annual_inc</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dti</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>funded_amnt</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>C</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>D</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>E</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>G</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NONE</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>OTHER</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>OWN</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>RENT</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>car</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>credit_card</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>educational</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>home_improvement</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>house</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>major_purchase</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>medical</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>moving</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>other</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>renewable_energy</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>small_business</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>vacation</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>wedding</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              features  label\n",
       "0             int_rate      0\n",
       "1           annual_inc      1\n",
       "2                  dti      2\n",
       "3          funded_amnt      3\n",
       "4                    A      4\n",
       "5                    B      5\n",
       "6                    C      6\n",
       "7                    D      7\n",
       "8                    E      8\n",
       "9                    F      9\n",
       "10                   G     10\n",
       "11            MORTGAGE     11\n",
       "12                NONE     12\n",
       "13               OTHER     13\n",
       "14                 OWN     14\n",
       "15                RENT     15\n",
       "16                 car     16\n",
       "17         credit_card     17\n",
       "18  debt_consolidation     18\n",
       "19         educational     19\n",
       "20    home_improvement     20\n",
       "21               house     21\n",
       "22      major_purchase     22\n",
       "23             medical     23\n",
       "24              moving     24\n",
       "25               other     25\n",
       "26    renewable_energy     26\n",
       "27      small_business     27\n",
       "28            vacation     28\n",
       "29             wedding     29"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = pd.DataFrame({'features':columns, 'label':range(len(columns))})\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then remove 20% of the features at a time in a ranked order based on GBM's feature importance metric then see how the new models perform using k-fold cross validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10) # Define the split - into 4\n",
    "all_trials = []\n",
    "for i in range(int(np.ceil(len(columns)/6))):\n",
    "    sorted_features = importance.argsort()[-(len(importance)-i*6):][::-1]\n",
    "    for index, (train, test) in enumerate(kf.split(df_train)):\n",
    "        x_train = df_train.iloc[train][columns].iloc[:,sorted_features]\n",
    "        y_train = df_train.iloc[train]['irr']\n",
    "        y_test = df_train.iloc[test]['irr']\n",
    "        x_test = df_train.iloc[test][columns].iloc[:,sorted_features]\n",
    "\n",
    "        params = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 2,\n",
    "              'learning_rate': 0.01, 'loss': 'ls'}\n",
    "\n",
    "        clf = ensemble.GradientBoostingRegressor(**params)\n",
    "        clf.fit(x_train, y_train)\n",
    "        gbm_predictions = clf.predict(x_test)\n",
    "        df_temp = pd.DataFrame({'irr':y_test, 'gbm_predictions':gbm_predictions})\n",
    "        gbm_returns = np.mean(df_temp.nlargest(1000, 'gbm_predictions')['irr'])\n",
    "        all_trials.append({\n",
    "            'n_features': len(x_train.columns),\n",
    "            'mse': calc_train_error(x_test,y_test,clf),\n",
    "            'returns': gbm_returns\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output=pd.DataFrame.from_records(all_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                 mse   returns\n",
       " n_features                    \n",
       " 6           0.045751  0.094097\n",
       " 12          0.045558  0.096423\n",
       " 18          0.045562  0.096966\n",
       " 24          0.045561  0.096399\n",
       " 30          0.045564  0.096527,                  mse   returns\n",
       " n_features                    \n",
       " 6           0.000005  0.000048\n",
       " 12          0.000005  0.000030\n",
       " 18          0.000005  0.000032\n",
       " 24          0.000005  0.000048\n",
       " 30          0.000005  0.000042)"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.groupby(['n_features'])['mse','returns'].mean() , output.groupby(['n_features'])['mse','returns'].var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not clear which option is the best however we can see that 18 features beat out the 30 features in both MSE and returns while only marginally losing in accuracy to others. Below are the features that made the cut for 18 features. Below we will train a model all the features available to us and see how that performs versus the vetted model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>int_rate</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>annual_inc</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dti</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>funded_amnt</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>C</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>D</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>E</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>G</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>car</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>credit_card</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>educational</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>major_purchase</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>medical</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>other</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>small_business</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              features  label\n",
       "0             int_rate      0\n",
       "1           annual_inc      1\n",
       "2                  dti      2\n",
       "3          funded_amnt      3\n",
       "5                    B      5\n",
       "6                    C      6\n",
       "7                    D      7\n",
       "8                    E      8\n",
       "9                    F      9\n",
       "10                   G     10\n",
       "16                 car     16\n",
       "17         credit_card     17\n",
       "18  debt_consolidation     18\n",
       "19         educational     19\n",
       "22      major_purchase     22\n",
       "23             medical     23\n",
       "25               other     25\n",
       "27      small_business     27"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[labels['label'].isin(importance.argsort()[-(18):][::-1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10) # Define the split - into 4\n",
    "all_features_trials = []\n",
    "for index, (train, test) in enumerate(kf.split(df_train)):\n",
    "    x_train = df_train.iloc[train][columns_all]\n",
    "    y_train = df_train.iloc[train]['irr']\n",
    "    y_test = df_train.iloc[test]['irr']\n",
    "    x_test = df_train.iloc[test][columns_all]\n",
    "\n",
    "    params = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 2,\n",
    "          'learning_rate': 0.01, 'loss': 'ls'}\n",
    "\n",
    "    clf = ensemble.GradientBoostingRegressor(**params)\n",
    "    clf.fit(x_train, y_train)\n",
    "    gbm_predictions = clf.predict(x_test)\n",
    "    df_temp = pd.DataFrame({'irr':y_test, 'gbm_predictions':gbm_predictions})\n",
    "    gbm_returns = np.mean(df_temp.nlargest(1000, 'gbm_predictions')['irr'])\n",
    "    all_features_trials.append({\n",
    "        'n_features': len(x_train.columns),\n",
    "        'mse': calc_train_error(x_test,y_test,clf),\n",
    "        'returns': gbm_returns\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                 mse   returns\n",
       " n_features                    \n",
       " 48          0.045504  0.099542,                  mse   returns\n",
       " n_features                    \n",
       " 48          0.000005  0.000039)"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output2=pd.DataFrame.from_records(all_features_trials)\n",
    "output2.groupby(['n_features'])['mse','returns'].mean() , output2.groupby(['n_features'])['mse','returns'].var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throwing in all the features into our model significantly outperformed the vetted model. This demonstrates the power of modern machine learning techniques. Although many of our features are clearly correlated and by traditionalist standards, often are discouraged from being used together, marginal predictive utility of each features added up to improve the prediction significantly. Also, the built-in regularization ability of GBM help curb the negative variance tradeoffs. As such, often, additional time invested in careful feature selection can sometimes make your model worse. With this confirmation in mind, I will employ all 48 features but improve the model further by tuning the parameters of the model in the next post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title: parameter tuning in Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last [post](http://jameslee.posthaven.com/peer-to-peer-lending-markets-part-3-feature-selection), methods for feature selection and its merits in modern machine learning were discussed. While feature selection is critical, methods such as Gradient Boosting has built-in regularization methods that can make aggressive pruning of features counterproductive. Incremental predictive utility from the features despite their short-comings such as multicollinearity add up to make a big difference in prediction performance. As a demonstration, a model with vetted features and a model with all the features were pitted against each other using performance samples from cross-validation. A model with all the features dominated the other. In this post, I will take it a step further with parameter tuning and see if it can outperform a regression model, which beat out the GBM model in this [post](http://jameslee.posthaven.com/peer-to-peer-lending-markets-part-2-using-linear-regression-and-gradient-boosting-regression-to-outperform-the-market).\n",
    "\n",
    "Hyperparameters are model specific parameters that dictate how the training should be done. It is important to get this step right to harness the full power of the method. We will use GridSearchCV function from Scikitlearn which exhaustively considers all possibilities of the parameters the user provides. In an ideal scenario, different combinations of the parameters are tuned together, but that requires a lot of computation resources. So, here, I will tune the parameters sequentially in the order of their importance.\n",
    "\n",
    "First, we choose a learning rate. Learning rate determines how fast or slow we travel down the downward slope in gradient descent with respect to the loss function. A lower learning rate will almost surely improve the model's performance but it requires more computation power.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn-images-1.medium.com/max/1600/0*00BrbBeDrFOjocpK.\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://cdn-images-1.medium.com/max/1600/0*00BrbBeDrFOjocpK.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: Professor Andrew Ng's Machine Learning Course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found .01 to be sufficiently low enough and doesn't take too long for my computation resources. As a reference, default value is .1. With our learning rate set in place, the number of estimators is tuned. This parameter dictates the number of trees that are added to the model. Generally, bigger number improvesperformance. GBM is robust to overfitting so a large number can be utilized but a tradeoff here is a longer computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"n_estimators\":[50, 100, 200, 500, 1000,2000]\n",
    "    }\n",
    "clf_ntrees = GridSearchCV(ensemble.GradientBoostingRegressor(learning_rate=0.01, min_samples_split=300,\n",
    "                                                      min_samples_leaf=50,max_depth=8,max_features='sqrt',\n",
    "                                                      subsample=0.8,random_state=10), parameters, cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_ntrees.fit(df_train[columns_all], df_train['irr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 500} 0.020952086331371023\n"
     ]
    }
   ],
   "source": [
    "print(clf_ntrees.best_params_, clf_ntrees.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we try a wide ranging number of estimators then we tune again more granularly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"n_estimators\": np.linspace(300, 1000,8).astype(int)\n",
    "    }\n",
    "clf_ntrees = GridSearchCV(ensemble.GradientBoostingRegressor(learning_rate=0.01, min_samples_split=300,\n",
    "                                                      min_samples_leaf=50,max_depth=8,max_features='sqrt',\n",
    "                                                      subsample=0.8,random_state=10), parameters, cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_ntrees.fit(df_train[columns_all], df_train['irr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 700} 0.021230898649948225\n"
     ]
    }
   ],
   "source": [
    "print(clf_ntrees.best_params_, clf_ntrees.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using 700 estimators maximizes our score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we continue to tune the variables with a higher impact on outcome first. Max depth and minimum number of samples dictate maximum depth of the individual regression estimators and minimum number of samples required at a leaf node respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters2= {\n",
    "    'max_depth':  np.linspace(4, 16, 7).astype(int), 'min_samples_split': np.linspace(200,1000, 5).astype(int)\n",
    "}\n",
    "clf_max_min = GridSearchCV(ensemble.GradientBoostingRegressor(learning_rate=0.01, \n",
    "                                                      min_samples_leaf=50,max_features='sqrt',\n",
    "                                                      subsample=0.8,random_state=10, n_estimators = 700), parameters2, cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_max_min.fit(df_train[columns_all], df_train['irr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 8, 'min_samples_split': 200} 0.021412471394592434\n"
     ]
    }
   ],
   "source": [
    "print(clf_max_min.best_params_, clf_max_min.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8 max_depth and 200 min_sample_split were the winning parameters. We can iterate more granularly while simultaneously tuning the next parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters3= {\n",
    "    'max_depth':  np.linspace(7,9,3).astype(int), 'min_samples_split': np.linspace(50,300,5).astype(int), 'min_samples_leaf': np.linspace(30,80,6).astype(int)\n",
    "}\n",
    "clf_max_min_leaves = GridSearchCV(ensemble.GradientBoostingRegressor(learning_rate=0.01, \n",
    "                                                      max_features='sqrt',\n",
    "                                                      subsample=0.8,random_state=10, n_estimators = 700), parameters3, cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_max_min_leaves.fit(df_train[columns_all], df_train['irr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'max_depth': 7, 'min_samples_leaf': 40, 'min_samples_split': 112},\n",
       " 0.021664220844255632)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_max_min_leaves.best_params_, clf_max_min_leaves.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue the same process for max_features and subsample parameters. Both of these parameters tune variance and bias tradeoffs of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters4= {\n",
    "    'max_features':  np.linspace(1,17,17).astype(int)\n",
    "}\n",
    "clf_max_min_leaves_feature = GridSearchCV(ensemble.GradientBoostingRegressor(learning_rate=0.01, \n",
    "                                                      max_depth = 7, min_samples_leaf = 40, min_samples_split = 112,\n",
    "                                                      subsample=0.8,random_state=10, n_estimators = 700), parameters4, cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_max_min_leaves_feature.fit(df_train[columns_all], df_train['irr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'max_features': 6}, 0.021664220844255632)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_max_min_leaves_feature.best_params_, clf_max_min_leaves_feature.best_score_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters5= {\n",
    "    'subsample':  np.linspace(.5,1,6)\n",
    "}\n",
    "clf_5 = GridSearchCV(ensemble.GradientBoostingRegressor(learning_rate=0.01, max_features=6,\n",
    "                                                      max_depth = 7, min_samples_leaf = 40, min_samples_split =112,\n",
    "                                                      random_state=10, n_estimators = 700), parameters5, cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.021664220844255632, {'subsample': 0.8})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_5.fit(df_train[columns_all], df_train['irr'])\n",
    "clf_5.best_score_ , clf_5.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all our parameters are tuned. We will take these parameters and train it over all our training set. Additionally, we will train the linear regression models, one with the same setup as the last post, and another with all the features. These models will be compared by looking at how accurate the predictions are and the returns of the loans they choose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_6 = ensemble.GradientBoostingRegressor(learning_rate=0.01, max_features=6, subsample=.8,\n",
    "                                                      max_depth = 7, min_samples_leaf = 40, min_samples_split =112,\n",
    "                                                      random_state=10, n_estimators = 700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_6.fit(df_train[columns_all], df_train['irr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = df_test['irr']\n",
    "x_test = df_test[columns_all]\n",
    "x_train = df_train[columns_all]\n",
    "y_train = df_train['irr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_predictions  = clf_6.predict(x_test)\n",
    "gbm_mse = mean_squared_error(y_test, gbm_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_features = [\"int_rate\",\"annual_inc\",\"funded_amnt\",\"F\",\"G\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "##training the modelsz\n",
    "lm = linear_model.LinearRegression()\n",
    "lm.fit(x_train, y_train)\n",
    "lm_predictions = lm.predict(x_test)\n",
    "lm_mse = mean_squared_error(y_test, lm_predictions)\n",
    "lm2 = linear_model.LinearRegression()\n",
    "lm2.fit(x_train[lm_features], y_train)\n",
    "lm2_predictions = lm2.predict(x_test[lm_features])\n",
    "lm2_mse = mean_squared_error(y_test, lm2_predictions)\n",
    "df_temp = pd.DataFrame({'irr':y_test, 'gbm_predictions':gbm_predictions, 'lm_predictions':lm_predictions,\n",
    "                        'lm2_predictions': lm2_predictions})\n",
    "lm_returns = np.mean(df_temp.nlargest(1000, 'lm_predictions')['irr'])\n",
    "lm2_returns = np.mean(df_temp.nlargest(1000, 'lm2_predictions')['irr'])\n",
    "gbm_returns= np.mean(df_temp.nlargest(1000, 'gbm_predictions')['irr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.12639511613653076, 0.046555722287982194)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm_returns= np.mean(df_temp.nlargest(100, 'gbm_predictions')['irr'])\n",
    "gbm_returns, gbm_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.10727884733922405, 0.04688804011489802)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_returns = np.mean(df_temp.nlargest(100, 'lm_predictions')['irr'])\n",
    "lm_returns, lm_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.10128407954424563, 0.0473454510718093)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm2_returns = np.mean(df_temp.nlargest(100, 'lm2_predictions')['irr'])\n",
    "lm2_returns, lm2_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our GBM model outperformed both variations of linear models, one with careful feature selection leveraging F-statistics, and one with all the features included. We can also gather a distribution of performance scores by sampling from our testing set instead of using all of it at once. This gives us insights, such as had we invested in 100 loans at a time using the output of these models as a guidance, what would the risk-return tradeoff of our portfolios looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_trials = []\n",
    "for i in range(1000):\n",
    "    df_sample = df_temp.sample(1000,replace=False)\n",
    "    gbm_returns= np.mean(df_sample.nlargest(100, 'gbm_predictions')['irr'])\n",
    "    lm_returns = np.mean(df_sample.nlargest(100, 'lm_predictions')['irr'])\n",
    "    lm2_returns = np.mean(df_sample.nlargest(100, 'lm2_predictions')['irr'])\n",
    "    returns_trials.append({\n",
    "        'index': i,\n",
    "        'gbm_returns': gbm_returns,\n",
    "        'lm_returns': lm_returns,\n",
    "        'lm2_returns': lm2_returns\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_output=pd.DataFrame.from_records(returns_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbm_returns: 0.1044967428863401 , linear regression returns: 0.09884446624456748 , linear regression2 returns: 0.08820359197787124\n"
     ]
    }
   ],
   "source": [
    "print('gbm_returns:', np.mean(returns_output['gbm_returns']),\n",
    "      ', linear regression returns:' ,np.mean(returns_output['lm_returns']),\n",
    "      ', linear regression2 returns:' ,np.mean(returns_output['lm2_returns']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "returns_output[['gbm_returns', 'lm_returns', 'lm2_returns']].boxplot(vert=False)\n",
    "plt.title('Model Return Distributions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, even with many more trials, returns of the portfolios constructed using our gbm model greatly outperformed the linear models. In this case, the improvement in prediction is strong enough to merit usage of GBM over simpler methods such as Linear Regression. Improvement in 1% return can translate into profound returns over time. In the next series of posts, I will investigate putting this model into work by building an automated investment algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
